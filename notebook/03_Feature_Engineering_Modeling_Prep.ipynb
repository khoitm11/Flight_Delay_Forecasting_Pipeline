{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.521097Z",
     "start_time": "2025-05-30T15:08:50.464680Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib # Để lưu/load preprocessor, model\n",
    "import json   # Để lưu thông tin features\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Cấu hình\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Định nghĩa đường dẫn\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "processed_data_folder = os.path.join(project_root, 'data', 'bts_processed')\n",
    "models_folder = os.path.join(project_root, 'saved_models') # Thư mục lưu model và scaler\n",
    "\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n",
    "    print('Đã tạo thư mục saved_models:', models_folder)\n",
    "\n",
    "# Load dữ liệu đã làm giàu từ EDA (02)\n",
    "input_filename_eda = 'flight_delay_2024_eda_enriched.csv'\n",
    "eda_enriched_file_path = os.path.join(processed_data_folder, input_filename_eda)\n",
    "\n",
    "df_model_input = None # Khởi tạo\n",
    "if os.path.exists(eda_enriched_file_path):\n",
    "    df_model_input = pd.read_csv(eda_enriched_file_path)\n",
    "    print('Đã load dữ liệu từ:', eda_enriched_file_path)\n",
    "    print('Shape của df_model_input:', df_model_input.shape)\n",
    "    display(df_model_input.head())\n",
    "else:\n",
    "    print('LỖI: Không tìm thấy file {}.'.format(eda_enriched_file_path))\n",
    "    print('Vui lòng chạy notebook EDA (Phần 2) và lưu file đúng cách trước.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load dữ liệu từ: C:\\Users\\hoiti\\PycharmProjects\\flight_delay_bts_project\\data\\bts_processed\\flight_delay_2024_eda_enriched.csv\n",
      "Shape của df_model_input: (5677, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  airport  arr_cancelled  arr_diverted  arr_flights carrier  carrier_ct  \\\n",
       "0     SGF            0.0           0.0        119.0      OH        5.50   \n",
       "1     SHV            0.0           0.0         92.0      OH        4.56   \n",
       "2     SRQ            7.0           0.0         31.0      OH        0.40   \n",
       "3     STL            0.0           0.0        115.0      OH        6.87   \n",
       "4     SYR            0.0           0.0         15.0      OH        1.42   \n",
       "\n",
       "   delay_rate  late_aircraft_ct  month  nas_ct  security_ct  weather_ct  year  \\\n",
       "0    0.134454              6.90     10    3.60          0.0         0.0  2024   \n",
       "1    0.130435              1.52     10    5.92          0.0         0.0  2024   \n",
       "2    0.096774              2.57     10    0.03          0.0         0.0  2024   \n",
       "3    0.130435              7.79     10    0.33          0.0         0.0  2024   \n",
       "4    0.266667              1.53     10    1.06          0.0         0.0  2024   \n",
       "\n",
       "   carrier_ct_rate  weather_ct_rate  nas_ct_rate  security_ct_rate  \\\n",
       "0         0.046218              0.0     0.030252               0.0   \n",
       "1         0.049565              0.0     0.064348               0.0   \n",
       "2         0.012903              0.0     0.000968               0.0   \n",
       "3         0.059739              0.0     0.002870               0.0   \n",
       "4         0.094667              0.0     0.070667               0.0   \n",
       "\n",
       "   late_aircraft_ct_rate  \n",
       "0               0.057983  \n",
       "1               0.016522  \n",
       "2               0.082903  \n",
       "3               0.067739  \n",
       "4               0.102000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport</th>\n",
       "      <th>arr_cancelled</th>\n",
       "      <th>arr_diverted</th>\n",
       "      <th>arr_flights</th>\n",
       "      <th>carrier</th>\n",
       "      <th>carrier_ct</th>\n",
       "      <th>delay_rate</th>\n",
       "      <th>late_aircraft_ct</th>\n",
       "      <th>month</th>\n",
       "      <th>nas_ct</th>\n",
       "      <th>security_ct</th>\n",
       "      <th>weather_ct</th>\n",
       "      <th>year</th>\n",
       "      <th>carrier_ct_rate</th>\n",
       "      <th>weather_ct_rate</th>\n",
       "      <th>nas_ct_rate</th>\n",
       "      <th>security_ct_rate</th>\n",
       "      <th>late_aircraft_ct_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>6.90</td>\n",
       "      <td>10</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.046218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>1.52</td>\n",
       "      <td>10</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRQ</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>2.57</td>\n",
       "      <td>10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>6.87</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.059739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.53</td>\n",
       "      <td>10</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.094667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.539921Z",
     "start_time": "2025-05-30T15:08:50.521097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.1. Feature Selection & Target Definition ---\n",
    "X = None\n",
    "y = None\n",
    "selected_features = []\n",
    "categorical_cols = [] # Đổi tên cho ngắn gọn\n",
    "numeric_cols = []     # Đổi tên cho ngắn gọn\n",
    "\n",
    "if df_model_input is not None and not df_model_input.empty:\n",
    "    target_variable_name = 'delay_rate'\n",
    "\n",
    "    # Define potential features based on EDA (Part 2)\n",
    "    potential_cats = ['month', 'carrier', 'airport']\n",
    "    potential_nums = [\n",
    "        'arr_flights', 'arr_cancelled', 'arr_diverted',\n",
    "        'carrier_ct_rate', 'weather_ct_rate', 'nas_ct_rate',\n",
    "        'security_ct_rate', 'late_aircraft_ct_rate'\n",
    "    ]\n",
    "\n",
    "    # Filter to existing columns only\n",
    "    categorical_cols = [col for col in potential_cats if col in df_model_input.columns]\n",
    "    numeric_cols = [col for col in potential_nums if col in df_model_input.columns]\n",
    "\n",
    "    selected_features = sorted(list(set(categorical_cols + numeric_cols))) # Ensure unique and sorted\n",
    "\n",
    "    # Critical check: Ensure target is NOT in features\n",
    "    if target_variable_name in selected_features:\n",
    "        selected_features.remove(target_variable_name)\n",
    "        print('WARNING: Target \"{}\" was in features, removed.'.format(target_variable_name))\n",
    "\n",
    "    # Final check for missing selected features or target in df_model_input\n",
    "    missing_selected_features = [f for f in selected_features if f not in df_model_input.columns]\n",
    "    if missing_selected_features:\n",
    "        print('CRITICAL: Selected features NOT FOUND in df_model_input: {}'.format(missing_selected_features))\n",
    "        # Potentially raise error or stop execution if this happens\n",
    "    elif target_variable_name not in df_model_input.columns:\n",
    "        print('CRITICAL: Target column \"{}\" NOT FOUND in df_model_input.'.format(target_variable_name))\n",
    "    else:\n",
    "        X = df_model_input[selected_features].copy()\n",
    "        y = df_model_input[target_variable_name].copy()\n",
    "        print('--- Features & Target Created ---')\n",
    "        print('  Selected Features ({}): {}'.format(len(selected_features), selected_features))\n",
    "        print('  Categorical Features for Preprocessing:', categorical_cols)\n",
    "        print('  Numeric Features for Preprocessing:', numeric_cols)\n",
    "        print('  X shape: {}, y shape: {}'.format(X.shape, y.shape))\n",
    "else:\n",
    "    print(\"Input DataFrame 'df_model_input' is empty or not loaded. Skipping feature selection.\")"
   ],
   "id": "2e9a8635b9504e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features & Target Created ---\n",
      "  Selected Features (11): ['airport', 'arr_cancelled', 'arr_diverted', 'arr_flights', 'carrier', 'carrier_ct_rate', 'late_aircraft_ct_rate', 'month', 'nas_ct_rate', 'security_ct_rate', 'weather_ct_rate']\n",
      "  Categorical Features for Preprocessing: ['month', 'carrier', 'airport']\n",
      "  Numeric Features for Preprocessing: ['arr_flights', 'arr_cancelled', 'arr_diverted', 'carrier_ct_rate', 'weather_ct_rate', 'nas_ct_rate', 'security_ct_rate', 'late_aircraft_ct_rate']\n",
      "  X shape: (5677, 11), y shape: (5677,)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.600468Z",
     "start_time": "2025-05-30T15:08:50.589397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.2. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = (None, None, None, None)\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    # shuffle=True (mặc định) là tốt để đảm bảo tính ngẫu nhiên\n",
    "    print('--- Train/Test Data Split ---')\n",
    "    print('  X_train: {}, y_train: {}'.format(X_train.shape, y_train.shape))\n",
    "    print('  X_test: {}, y_test: {}'.format(X_test.shape, y_test.shape))\n",
    "else:\n",
    "    print(\"X or y is None. Skipping train-test split.\")"
   ],
   "id": "eff92fa50c532639",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train/Test Data Split ---\n",
      "  X_train: (4541, 11), y_train: (4541,)\n",
      "  X_test: (1136, 11), y_test: (1136,)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.702896Z",
     "start_time": "2025-05-30T15:08:50.648029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.3. Feature Preprocessing (Encoding & Scaling) ---\n",
    "X_train_processed = None\n",
    "X_test_processed = None\n",
    "preprocessor_obj = None # Đổi tên biến cho nhất quán\n",
    "final_feature_names = None # List of feature names AFTER preprocessing\n",
    "\n",
    "if X_train is not None and X_test is not None:\n",
    "    # These lists (categorical_cols, numeric_cols) should be defined and filtered in Cell 2\n",
    "    # based on df_model_input and then further filtered based on X_train.columns if needed,\n",
    "    # but if X comes directly from df_model_input[selected_features], they should match.\n",
    "\n",
    "    # Double check: use columns present in X_train for pipeline\n",
    "    cats_for_pipe = [col for col in categorical_cols if col in X_train.columns]\n",
    "    nums_for_pipe = [col for col in numeric_cols if col in X_train.columns]\n",
    "\n",
    "    # Define transformers\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    # Create ColumnTransformer\n",
    "    preprocessor_obj = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_transform', numerical_transformer, nums_for_pipe),\n",
    "            ('cat_transform', categorical_transformer, cats_for_pipe)\n",
    "        ],\n",
    "        remainder='drop' # Explicitly drop other columns (should be none if selected_features is comprehensive)\n",
    "    )\n",
    "\n",
    "    # Fit preprocessor on X_train and transform X_train\n",
    "    print('Fitting preprocessor on X_train...')\n",
    "    X_train_processed = preprocessor_obj.fit_transform(X_train)\n",
    "\n",
    "    # Transform X_test using the fitted preprocessor\n",
    "    print('Transforming X_test...')\n",
    "    X_test_processed = preprocessor_obj.transform(X_test)\n",
    "    print('Preprocessing complete.')\n",
    "\n",
    "    # Attempt to get feature names after transformation\n",
    "    try:\n",
    "        ohe_cols = list(preprocessor_obj.named_transformers_['cat_transform']\n",
    "                        .get_feature_names_out(cats_for_pipe))\n",
    "        final_feature_names = nums_for_pipe + ohe_cols\n",
    "        print('  Total features after preprocessing: {}'.format(len(final_feature_names)))\n",
    "        # print('  Example processed feature names (first 5):', final_feature_names[:5]) # Optional log\n",
    "    except Exception as e:\n",
    "        print('  Note: Could not retrieve feature names after OHE: {}.'.format(e))\n",
    "        print('        X_train_processed and X_test_processed will be NumPy arrays without column names attached.')\n",
    "        final_feature_names = None # Ensure it's None if failed\n",
    "\n",
    "    print('  X_train_processed shape: {}'.format(X_train_processed.shape))\n",
    "    print('  X_test_processed shape: {}'.format(X_test_processed.shape))\n",
    "else:\n",
    "    print(\"Train/Test data not available. Skipping preprocessing.\")"
   ],
   "id": "2a544cb3f8384396",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor on X_train...\n",
      "Transforming X_test...\n",
      "Preprocessing complete.\n",
      "  Total features after preprocessing: 385\n",
      "  X_train_processed shape: (4541, 385)\n",
      "  X_test_processed shape: (1136, 385)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.846021Z",
     "start_time": "2025-05-30T15:08:50.828011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3.4. Save Preprocessor and Feature Info ---\n",
    "if preprocessor_obj is not None and X_train is not None: # Ensure preprocessor was created and X_train exists\n",
    "    preprocessor_file = 'flight_data_preprocessor.joblib'\n",
    "    preprocessor_save_path = os.path.join(models_folder, preprocessor_file)\n",
    "    joblib.dump(preprocessor_obj, preprocessor_save_path)\n",
    "    print('Preprocessor saved to:', preprocessor_save_path)\n",
    "\n",
    "    # Save feature information used for THIS preprocessor\n",
    "    # 'cats_for_pipe' and 'nums_for_pipe' are what ColumnTransformer actually saw.\n",
    "    feature_info_dict = {\n",
    "        'numeric_features_in_pipe': nums_for_pipe,       # Columns fed to numerical_transformer\n",
    "        'categorical_features_in_pipe': cats_for_pipe, # Columns fed to categorical_transformer\n",
    "        'original_X_train_columns_order': list(X_train.columns), # Order of columns in X_train when preprocessor was fit\n",
    "        'processed_feature_names_order': final_feature_names if final_feature_names else 'Not available'\n",
    "    }\n",
    "    feature_info_file = 'preprocessor_feature_config.json' # More descriptive name\n",
    "    feature_info_save_path = os.path.join(models_folder, feature_info_file)\n",
    "    try:\n",
    "        with open(feature_info_save_path, 'w') as f:\n",
    "            json.dump(feature_info_dict, f, indent=4)\n",
    "        print('Preprocessor feature configuration saved to:', feature_info_save_path)\n",
    "    except Exception as e:\n",
    "        print('Error saving feature_info.json: {}'.format(e))\n",
    "else:\n",
    "    if preprocessor_obj is None: print(\"Preprocessor not created. Skipping save.\")\n",
    "    if X_train is None: print(\"X_train not available. Cannot save feature info related to it.\")\n",
    "\n",
    "print('\\n--- END OF PART 3: DATA PREPARATION FOR MODELING COMPLETE ---')"
   ],
   "id": "abac0630288c1823",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved to: C:\\Users\\hoiti\\PycharmProjects\\flight_delay_bts_project\\saved_models\\flight_data_preprocessor.joblib\n",
      "Preprocessor feature configuration saved to: C:\\Users\\hoiti\\PycharmProjects\\flight_delay_bts_project\\saved_models\\preprocessor_feature_config.json\n",
      "\n",
      "--- END OF PART 3: DATA PREPARATION FOR MODELING COMPLETE ---\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.873934Z",
     "start_time": "2025-05-30T15:08:50.848072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Modeling & MLflow ---\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np # Dùng cho np.sqrt\n",
    "\n",
    "# MLflow: set experiment\n",
    "# Sẽ tạo thư mục 'mlruns' ở root project nếu chưa có\n",
    "experiment_name = 'Flight_Delay_BTS'\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print('MLflow experiment set to: {}'.format(experiment_name))\n",
    "except Exception as e:\n",
    "    print('MLflow set_experiment error: {}. Ensure MLflow server is configured if not using local.'.format(e))"
   ],
   "id": "474cb7ca4471a4fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment set to: Flight_Delay_BTS\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:50.992309Z",
     "start_time": "2025-05-30T15:08:50.984683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7.1. Verify data from Part 3 is ready and valid\n",
    "data_is_truly_ready = False\n",
    "mlflow_input_example_final = None\n",
    "\n",
    "if ('X_train_processed' in locals() and isinstance(X_train_processed, np.ndarray) and\n",
    "    'y_train' in locals() and isinstance(y_train, pd.Series) and\n",
    "    'X_test_processed' in locals() and isinstance(X_test_processed, np.ndarray) and\n",
    "    'y_test' in locals() and isinstance(y_test, pd.Series) and\n",
    "    X_train_processed.shape[0] == y_train.shape[0] and X_test_processed.shape[0] == y_test.shape[0] and\n",
    "    X_train_processed.shape[1] == X_test_processed.shape[1] and X_train_processed.shape[1] > 0):\n",
    "\n",
    "    data_is_truly_ready = True\n",
    "    print('VERIFIED: Data (X_processed, y) looks structurally ready for modeling.')\n",
    "    print('  Shape X_train_processed: {} (Features: {})'.format(X_train_processed.shape, X_train_processed.shape[1]))\n",
    "\n",
    "    # 7.2. Prepare input_example for MLflow (prioritize NumPy if X_train_processed is NumPy)\n",
    "    num_ex_rows = min(5, X_train_processed.shape[0])\n",
    "    mlflow_input_example_final = X_train_processed[:num_ex_rows] # Default to NumPy array\n",
    "    # print('  mlflow_input_example_final created as NumPy array.') # Optional log\n",
    "\n",
    "    # Optionally, if feature names are reliable AND X_train_processed was meant to be DataFrame when fitted\n",
    "    # This part is tricky because preprocessor usually outputs NumPy. For now, stick to NumPy for input_example\n",
    "    # if 'processed_feature_names_list' in locals() and processed_feature_names_list and len(processed_feature_names_list) == X_train_processed.shape[1]:\n",
    "    #     mlflow_input_example_final = pd.DataFrame(X_train_processed[:num_ex_rows], columns=processed_feature_names_list)\n",
    "    #     print('  mlflow_input_example_final updated to DataFrame (with column names).')\n",
    "\n",
    "else:\n",
    "    print('CRITICAL FAILURE: Input data from Part 3 is NOT valid or missing.')\n",
    "    print('  >>> CHECK PART 3 CELLS (Split, Preprocessing) THOROUGHLY. <<<')"
   ],
   "id": "dac600285651dfb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFIED: Data (X_processed, y) looks structurally ready for modeling.\n",
      "  Shape X_train_processed: (4541, 385) (Features: 385)\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:51.069015Z",
     "start_time": "2025-05-30T15:08:51.052089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os # Needed for path joining later, e.g. saving models/plots\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "print('Phase 4 Init (Extreme Simplicity): Modeling & MLflow libs ready.')\n",
    "\n",
    "# MLflow Experiment\n",
    "mlflow_experiment_extreme_run = 'Flight_Delay_BTS_V1' # New distinct name\n",
    "try:\n",
    "    current_experiment_info = mlflow.set_experiment(mlflow_experiment_extreme_run)\n",
    "    print(f'MLflow experiment for Extreme Simplicity Run: \"{current_experiment_info.name}\"')\n",
    "except Exception as e:\n",
    "    print(f'MLflow set_experiment error: {e}. Ensure MLflow server is running and configured.')\n",
    "\n",
    "# 6.3. Suppress specific warnings globally for cleaner output (use with caution)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.utils.validation\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mlflow.sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\") # For potential seaborn plot warnings\n",
    "print(\"Global UserWarning (sklearn, mlflow) and FutureWarning (seaborn) suppression enabled for cleaner P4 output.\")"
   ],
   "id": "df6570f0a4cc81ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 4 Init (Extreme Simplicity): Modeling & MLflow libs ready.\n",
      "MLflow experiment for Extreme Simplicity Run: \"Flight_Delay_BTS_V1\"\n",
      "Global UserWarning (sklearn, mlflow) and FutureWarning (seaborn) suppression enabled for cleaner P4 output.\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:08:51.081449Z",
     "start_time": "2025-05-30T15:08:51.069015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8.1. Define the training, evaluation, and logging function (Extreme Simplicity)\n",
    "def execute_model_pipeline_extreme(\n",
    "    model_instance,\n",
    "    run_name_suffix,\n",
    "    xtr, ytr, xte, yte, # Assumed to be valid and existing\n",
    "    hyperparameters=None,\n",
    "    mlflow_signature_input_ex=None # Renamed for clarity in extreme version\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains, evaluates, and logs a model to MLflow. Assumes data is ready.\n",
    "    \"\"\"\n",
    "    full_run_name = f\"{model_instance.__class__.__name__}_{run_name_suffix}\"\n",
    "    print(f\"\\nAttempting to train: {full_run_name}\")\n",
    "\n",
    "    with mlflow.start_run(run_name=full_run_name) as active_run:\n",
    "        if hyperparameters:\n",
    "            try:\n",
    "                if hasattr(model_instance, 'set_params'): # Check before calling\n",
    "                    model_instance.set_params(**hyperparameters)\n",
    "                mlflow.log_params(hyperparameters)\n",
    "            except Exception as e_param:\n",
    "                print(f\"  Warning: Param issue for {full_run_name}. Error: {e_param}\")\n",
    "\n",
    "        model_instance.fit(xtr, ytr)\n",
    "        predictions = model_instance.predict(xte)\n",
    "\n",
    "        mae = mean_absolute_error(yte, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(yte, predictions))\n",
    "        r2 = r2_score(yte, predictions)\n",
    "\n",
    "        print(f\"  RESULTS for {full_run_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(\"test_mae\", mae)\n",
    "        mlflow.log_metric(\"test_rmse\", rmse)\n",
    "        mlflow.log_metric(\"test_r2\", r2)\n",
    "\n",
    "        model_artifact_name = f\"{model_instance.__class__.__name__.lower()}_model_extreme\"\n",
    "        if mlflow_signature_input_ex is not None:\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model_instance,\n",
    "                artifact_path=model_artifact_name,\n",
    "                input_example=mlflow_signature_input_ex\n",
    "            )\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model_instance,\n",
    "                artifact_path=model_artifact_name\n",
    "            )\n",
    "        return model_instance"
   ],
   "id": "fe7e62f38f85f3bf",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:09:21.485025Z",
     "start_time": "2025-05-30T15:08:51.174824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9.1. Prepare MLflow input example locally (as Cell 7 was removed)\n",
    "# This MUST succeed if X_train_processed is valid from Part 3.\n",
    "mlflow_input_example_for_signature = None\n",
    "try:\n",
    "    # Crucial: X_train_processed, final_feature_names must exist from Part 3\n",
    "    # If X_train_processed is None or empty, this will be skipped.\n",
    "    if 'X_train_processed' in locals() and X_train_processed is not None and X_train_processed.shape[0] > 0:\n",
    "        num_rows_example = min(5, X_train_processed.shape[0])\n",
    "        sample_data_example = X_train_processed[:num_rows_example]\n",
    "\n",
    "        if ('final_feature_names' in locals() and\n",
    "            isinstance(final_feature_names, list) and\n",
    "            len(final_feature_names) == X_train_processed.shape[1]):\n",
    "            mlflow_input_example_for_signature = pd.DataFrame(sample_data_example, columns=final_feature_names)\n",
    "            print('Cell 9: Created DataFrame input_example for MLflow signature.')\n",
    "        else:\n",
    "            mlflow_input_example_for_signature = sample_data_example\n",
    "            print('Cell 9: Created NumPy array input_example for MLflow (final_feature_names not suitable/available).')\n",
    "    else:\n",
    "        print(\"Cell 9 WARNING: X_train_processed not found or empty. Models will be logged without input_example.\")\n",
    "except NameError as ne: # Handles cases where X_train_processed or final_feature_names might not be defined\n",
    "    print(f\"Cell 9 WARNING: Could not create input_example due to missing variable ({ne}). Models logged without it.\")\n",
    "except Exception as e_ex_create:\n",
    "    print(f\"Cell 9 WARNING: Error creating input_example: {e_ex_create}. Models logged without it.\")\n",
    "\n",
    "\n",
    "# 9.2. Train various models (Extreme Simplicity)\n",
    "# ASSUMPTION: X_train_processed, y_train, X_test_processed, y_test are valid and exist from Part 3.\n",
    "# If not, this cell will raise an error directly.\n",
    "print(\"\\n---=== BATCH MODEL TRAINING (EXTREME SIMPLICITY RUN) ===---\")\n",
    "extreme_run_trained_models = {}\n",
    "\n",
    "# Common arguments for model execution\n",
    "# These will cause NameError if Part 3 variables are not set.\n",
    "common_args_extreme = {\n",
    "    \"xtr\": X_train_processed, \"ytr\": y_train,\n",
    "    \"xte\": X_test_processed, \"yte\": y_test,\n",
    "    \"mlflow_signature_input_ex\": mlflow_input_example_for_signature\n",
    "}\n",
    "\n",
    "# Model 1: Linear Regression\n",
    "lr_extreme_model = LinearRegression()\n",
    "extreme_run_trained_models['LinearRegression'] = execute_model_pipeline_extreme(\n",
    "    model_instance=lr_extreme_model, run_name_suffix=\"Baseline_Extreme\", **common_args_extreme\n",
    ")\n",
    "\n",
    "# Model 2: Ridge Regression\n",
    "ridge_extreme_params = {'alpha': 1.0}\n",
    "ridge_extreme_model = Ridge() # Params set by execute_model_pipeline_extreme\n",
    "extreme_run_trained_models['Ridge'] = execute_model_pipeline_extreme(\n",
    "    model_instance=ridge_extreme_model, run_name_suffix=f\"Alpha{ridge_extreme_params['alpha']}_Extreme\",\n",
    "    hyperparameters=ridge_extreme_params, **common_args_extreme\n",
    ")\n",
    "\n",
    "# Model 3: Lasso Regression\n",
    "lasso_extreme_params = {'alpha': 0.001, 'max_iter': 10000}\n",
    "lasso_extreme_model = Lasso()\n",
    "extreme_run_trained_models['Lasso'] = execute_model_pipeline_extreme(\n",
    "    model_instance=lasso_extreme_model, run_name_suffix=f\"Alpha{lasso_extreme_params['alpha']}_Extreme\",\n",
    "    hyperparameters=lasso_extreme_params, **common_args_extreme\n",
    ")\n",
    "\n",
    "# Model 4: Random Forest Regressor (Simplified params for speed)\n",
    "rf_extreme_params = {'n_estimators': 50, 'max_depth': 8, 'random_state': 42, 'n_jobs': -1}\n",
    "rf_extreme_model = RandomForestRegressor()\n",
    "extreme_run_trained_models['RandomForest'] = execute_model_pipeline_extreme(\n",
    "    model_instance=rf_extreme_model, run_name_suffix=\"QuickRF_Extreme\",\n",
    "    hyperparameters=rf_extreme_params, **common_args_extreme\n",
    ")\n",
    "\n",
    "# Model 5: Gradient Boosting Regressor (Simplified params for speed)\n",
    "gbr_extreme_params = {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 4, 'random_state': 42}\n",
    "gbr_extreme_model = GradientBoostingRegressor()\n",
    "extreme_run_trained_models['GradientBoosting'] = execute_model_pipeline_extreme(\n",
    "    model_instance=gbr_extreme_model, run_name_suffix=\"QuickGBR_Extreme\",\n",
    "    hyperparameters=gbr_extreme_params, **common_args_extreme\n",
    ")\n",
    "\n",
    "print(\"\\n---=== FINISHED BATCH MODEL TRAINING (EXTREME SIMPLICITY RUN) ===---\")\n",
    "extreme_run_trained_models = {k: v for k, v in extreme_run_trained_models.items() if v is not None} # Filter out failed trainings\n",
    "if not extreme_run_trained_models:\n",
    "    print(\"CRITICAL WARNING: No models were successfully trained in the extreme run. Check Part 3 outputs and data.\")\n",
    "else:\n",
    "    print(f\"Successfully trained models (extreme run): {list(extreme_run_trained_models.keys())}\")"
   ],
   "id": "5c14fd6838aa0001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 9: Created DataFrame input_example for MLflow signature.\n",
      "\n",
      "---=== BATCH MODEL TRAINING (EXTREME SIMPLICITY RUN) ===---\n",
      "\n",
      "Attempting to train: LinearRegression_Baseline_Extreme\n",
      "  RESULTS for LinearRegression_Baseline_Extreme: MAE=0.0001, RMSE=0.0001, R2=1.0000\n",
      "\n",
      "Attempting to train: Ridge_Alpha1.0_Extreme\n",
      "  RESULTS for Ridge_Alpha1.0_Extreme: MAE=0.0001, RMSE=0.0001, R2=1.0000\n",
      "\n",
      "Attempting to train: Lasso_Alpha0.001_Extreme\n",
      "  RESULTS for Lasso_Alpha0.001_Extreme: MAE=0.0015, RMSE=0.0027, R2=0.9993\n",
      "\n",
      "Attempting to train: RandomForestRegressor_QuickRF_Extreme\n",
      "  RESULTS for RandomForestRegressor_QuickRF_Extreme: MAE=0.0124, RMSE=0.0330, R2=0.8907\n",
      "\n",
      "Attempting to train: GradientBoostingRegressor_QuickGBR_Extreme\n",
      "  RESULTS for GradientBoostingRegressor_QuickGBR_Extreme: MAE=0.0084, RMSE=0.0286, R2=0.9180\n",
      "\n",
      "---=== FINISHED BATCH MODEL TRAINING (EXTREME SIMPLICITY RUN) ===---\n",
      "Successfully trained models (extreme run): ['LinearRegression', 'Ridge', 'Lasso', 'RandomForest', 'GradientBoosting']\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T15:09:21.495801Z",
     "start_time": "2025-05-30T15:09:21.485025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHOSEN_MODEL_KEY = 'GradientBoosting'\n",
    "chosen_gbr_r2 = 0.9180\n",
    "chosen_gbr_rmse = 0.0286\n",
    "\n",
    "if CHOSEN_MODEL_KEY in extreme_run_trained_models and \\\n",
    "   'model' in extreme_run_trained_models[CHOSEN_MODEL_KEY]:\n",
    "\n",
    "    gbr_model_to_save = extreme_run_trained_models[CHOSEN_MODEL_KEY]['model']\n",
    "    save_path = Path(models_folder) / f\"best_model_gbr.joblib\"\n",
    "\n",
    "    try:\n",
    "        joblib.dump(gbr_model_to_save, save_path)\n",
    "    except Exception as e_joblib:\n",
    "        print(f\"JOB LIB SAVE ERROR: {e_joblib}\")\n",
    "\n",
    "    try:\n",
    "        exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        if exp:\n",
    "            with mlflow.start_run(\n",
    "                run_name=f\"Best_{CHOSEN_MODEL_KEY}\",\n",
    "                experiment_id=exp.experiment_id,\n",
    "                log_system_metrics=False\n",
    "            ) as final_run:\n",
    "                mlflow.set_tag(\"is_best\", \"true\")\n",
    "                mlflow.log_params({\"final_model_type\": CHOSEN_MODEL_KEY})\n",
    "                mlflow.log_metrics({\"r2\": chosen_gbr_r2, \"rmse\": chosen_gbr_rmse})\n",
    "                mlflow.sklearn.log_model(\n",
    "                    sk_model=gbr_model_to_save,\n",
    "                    artifact_path=\"final_model_package\",\n",
    "                    input_example=mlflow_input_example_for_signature,\n",
    "                )\n",
    "    except Exception as e_mlflow:\n",
    "        print(f\"MLFLOW LOGGING ERROR: {e_mlflow}\")"
   ],
   "id": "7f8897e705998e86",
   "outputs": [],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
